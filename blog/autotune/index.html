<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.6">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="scvi-tools Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="scvi-tools Blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-141905405-3","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><title data-react-helmet="true">Hyperparameter search for scVI | scvi-tools</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://scvi-tools.org/blog/autotune"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Hyperparameter search for scVI | scvi-tools"><meta data-react-helmet="true" name="description" content="A study of the effects of hyperparameter optimization on scVI models."><meta data-react-helmet="true" property="og:description" content="A study of the effects of hyperparameter optimization on scVI models."><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2019-07-05T00:00:00.000Z"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://scvi-tools.org/blog/autotune"><link data-react-helmet="true" rel="alternate" href="https://scvi-tools.org/blog/autotune" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://scvi-tools.org/blog/autotune" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.34d598f1.css">
<link rel="preload" href="/assets/js/runtime~main.9e07ed31.js" as="script">
<link rel="preload" href="/assets/js/main.3bbe1f1b.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="scvi-tools Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/img/logo.svg" alt="scvi-tools Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">scvi-tools</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/get_started">Get Started</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link">Docs</a><ul class="dropdown__menu"><li><a href="https://docs.scvi-tools.org" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>Full documentation<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/tutorials/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>Tutorials<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/user_guide/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>User guide<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/api/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>API reference<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link">About</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/team">Team</a></li><li><a class="dropdown__link" href="/press">Press</a></li><li><a class="dropdown__link" href="/ecosystem">Ecosystem</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a href="https://discourse.scvi-tools.org/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Discussion<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://github.com/YosefLab/scvi-tools" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_2ahu thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_2hhb margin-bottom--md">Recent posts</div><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/destvi-batchsize">Mini-batch size in destVI</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/v090">scvi-tools 0.9.0 release</a></li><li class="sidebarItem_2UVv"><a aria-current="page" class="sidebarItemLink_1RT6 sidebarItemLinkActive_12pM" href="/blog/autotune">Hyperparameter search for scVI</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/zero-inflation">Should we zero-inflate scVI?</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_GeHD" itemprop="headline">Hyperparameter search for scVI</h1><div class="blogPostData_291c margin-vert--md"><time datetime="2019-07-05T00:00:00.000Z" itemprop="datePublished">July 5, 2019</time> · 20 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Gabriel Misrachi, Jeffrey Regier, Romain Lopez, Nir Yosef</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>While stochastic gradient-based optimization is highly successful for setting weights and other differentiable parameters of a neural network, it is in general useless for setting hyperparameters -- non-differentiable parameters that control the structure of the network (e.g. the number of hidden layers, or the dropout rate) or settings of the optimizer itself (e.g., the learning rate schedule). Yet finding good settings for hyperparameters is essential for good performance for deep methods like <a href="https://www.nature.com/articles/s41592-018-0229-2" target="_blank" rel="noopener noreferrer">scVI</a>. Furthermore, as pointed out by <a href="https://www.worldscientific.com/doi/pdf/10.1142/9789813279827_0033?download=true&amp;" target="_blank" rel="noopener noreferrer">Hu and Greene (2019)</a> selecting hyperparameters is nessary in order to compare different machine learning models, especially if those are substantially sensitive to hyperparameter variations.</p><p>Generally, hyperparameter search is known to be challenging for the end-user and time-consuming. Therefore, we added in <a href="https://github.com/YosefLab/scVI" target="_blank" rel="noopener noreferrer"><code>scVI</code></a> a module based on the Bayesian optimization framework <a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener noreferrer"><code>hyperopt</code></a>. This new feature makes effective use of multiple GPUs and is a ready-to-use solution for tuning hyperparameters in scVI. This blog post presents this feature and investigates how end-users will benefit from it.</p><header><h1>Theory : Bayesian Optimization and <code>Hyperopt</code></h1></header><p>First and foremost, one needs to carefully select a metric for which to search for optimal hyperparameters, keeping in mind that this one might depend on the downstream task, as explained in <a href="https://arxiv.org/abs/1511.01844" target="_blank" rel="noopener noreferrer">Theis et al. (2016)</a>. Since we train a generative model, the held-out negative log-likelihood is a natural criterion for model selection. We therefore rely on it for hyperparameters tuning.</p><p>Historically, the very first methods introduced to tackle hyperparameter search were grid search and random search. Such approaches are limited in the sense that they scale exponentially with the size of the search space. A more scalable and elegant approach to search is Bayesian Optimization (BO), whose main idea is to learn and use the structure of the objective function. The overall process can be decomposed into two unordered tasks. One is to define a tractable surrogate prior for the underlying black-box objective (e.g., a Gaussian Process). The other is to define a criterion for which to optimize this surrogate process.</p><p>In the special case of <code>hyperopt</code> the criterion used is the expectation of the improvement over a certain baseline which can be the current best or lower. The surrogate is called a Tree-strctured Parzen Estimator. The TPE divides the sample set into two subsets based on their value compared to the current baseline and estimates two corresponding conditional densities over the search space using non-parametric kernel-density estimators. Then, good samples should have a high conditional density for improving samples and vice versa.</p><p>As suggested in the original article <a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" target="_blank" rel="noopener noreferrer">Bergsta et al. (2011)</a>, this hyper-optimization process can be used asynchronously by mocking the result for the pending trials with the mean of the available samples. Still, it should be noted that if the budget is a single evaluation per process running in parallel, then a random search should be preferred.</p><p>There are a certain number of recent approaches competing with <code>hyperopt</code>. <a href="https://arxiv.org/abs/1603.06560" target="_blank" rel="noopener noreferrer">Li et al. (2018)</a> proposes Hyperband: an alternative to BO with bandit-based principled early stopping, which scales better with the number of workers.  These can be also combined for trial selection and principled early stopping, as explained in <a href="https://arxiv.org/abs/1801.01596" target="_blank" rel="noopener noreferrer">Wang et al. (2018)</a>. Interested readers might also take a look at two black-box optimizers based on different paradigms, namely the <a href="https://en.wikipedia.org/wiki/CMA-ES" target="_blank" rel="noopener noreferrer">CMA-ES</a> and <a href="https://github.com/TheClimateCorporation/dfo-algorithm" target="_blank" rel="noopener noreferrer">DFO-TR</a> algorithms - both have available code. BO enthusiasts can take a look at <a href="https://github.com/scikit-optimize/scikit-optimize" target="_blank" rel="noopener noreferrer">scikit-optimize</a> as well as <a href="https://github.com/facebook/Ax" target="_blank" rel="noopener noreferrer">Ax</a>, Facebook&#x27;s brand new pytorch-powered ML platform .</p><header><h1>Practice :  scVI&#x27;s autotune module</h1></header><p>We are excited to release our new <code>autotune</code> module for scVI, based on <code>hyperopt</code>. We thank <code>hyperopt</code> developers for their maintained codebase as well as their intelligent MongoDb-based communication between workers. Using this new feature on the Cortex dataset is as simple as running the code snippet below.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> logging</span></span><span class="token-line" style="color:#393A34"><span class="token plain">logger </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> logging</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">getLogger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;scvi.inference.autotune&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">logger</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">setLevel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">logging</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">DEBUG</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;__main__&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">    best_trainer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> trials </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> auto_tune_scvi_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">        gene_dataset</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">dataset</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">        parallel</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">        exp_key</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;cortex_test&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_evals</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Note that the default behaviour of <code>auto_tune_scvi_model</code> is as follows:</p><ul><li>A parameter search space containing the learning rate, the network architecture and the dropout rate is provided.</li><li>The objective metric is set to be the held-out log-likelihood of the data computed on a test set which size equals 25% of the whole dataset.</li><li>All available GPUs are used.</li></ul><p>Of course, the behaviour of the auto-tuning process can be widely customized. For example, one can select the number of CPU processes, the ids of the GPUs to use, one can also input their own hyperparameter search space or even their own objective function.  For a more advanced usage, we expose functions to launch your own workers, which could be used to run the search on multiple machines.</p><header><h1>Results</h1></header><p>In order to evaluate the autotune procedure, we ran it on several standard datasets, directly available using our codebase. Namely, we used the Cortex, Pbmc and Brain Large datasets.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="datasets"></a>Datasets<a class="hash-link" href="#datasets" title="Direct link to heading">#</a></h3><p>Let us first describe these three datasets in more detail.</p><ul><li><strong>CORTEX</strong> The mouse cortex dataset from <a href="https://science.sciencemag.org/content/347/6226/1138" target="_blank" rel="noopener noreferrer">Zeisel et al. (2015)</a> contains 3005 mouse cortex cells and gold-standard labels for seven distinct cell types. We retain the top 558 genes ordered by variance.</li><li><strong>PBMC</strong> As in the scVI manuscript, we concatenated two datasets of peripheral blood mononuclear cells (PBMCs) from a healthy donor from <a href="https://www.nature.com/articles/ncomms14049" target="_blank" rel="noopener noreferrer">Zheng et al. (2017)</a>. After filtering, we extract 11,990 cells with 3446 genes.</li><li><strong>BRAIN LARGE</strong> This dataset contains 1.3 million brain cells from <a href="https://support.10xgenomics.com/single-cell-gene-expression/datasets" target="_blank" rel="noopener noreferrer">10x Genomics</a>. We retain the 720 most variable genes.</li></ul><p>For each of these, we ran our autotune process using a default search space and a budget of 100 trainings. The raw outcome of these experiments can be found <a href="https://docs.google.com/spreadsheets/d/1IEKDr1bJcNjcnFbitmN8EVPizMzfWQUdiQIfE5hVtCw/edit?usp=sharing" target="_blank" rel="noopener noreferrer">here</a> in the form of a table containing the parameters used and the resulting performance.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="runtime-information"></a>Runtime information<a class="hash-link" href="#runtime-information" title="Direct link to heading">#</a></h3><p>We report the runtime metadata of our experiments in Table 1. This should give the user an idea of the time it takes to use our new feature, depending on the size of the dataset, the number of evaluations, the max number of epochs allowed and the number of GPUs available.</p><table><thead><tr><th>Dataset Name</th><th>Nb cells</th><th>Nb genes</th><th>Wall time</th><th>Avg epochs</th><th>N° of GPUs</th><th>Max epoch</th></tr></thead><tbody><tr><td>cortex</td><td>3005</td><td>558</td><td>9 hours</td><td>532</td><td>1</td><td>1000</td></tr><tr><td>pbmc</td><td>11990</td><td>3346</td><td>3 hours</td><td>387</td><td>16</td><td>1000</td></tr><tr><td>brain large</td><td>1303182</td><td>720</td><td>21.5 hours</td><td>43</td><td>16</td><td>50</td></tr></tbody></table><p>Table 1: Runtime Table for 100 trainings. Wall time is the total time the experiment took. Avg epoch is the average number of epochs per training.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="hyperparameter-sensitivity"></a>Hyperparameter sensitivity<a class="hash-link" href="#hyperparameter-sensitivity" title="Direct link to heading">#</a></h4><p>Our first concern was to investigate scVI&#x27;s sensitivity to hyperparameters. One ideal way to investigate this matter would have been to study the variance of the performance for each set of hyperparameters and collapse the runs who are not significantly different before analyzing the remaining representants. As this is too computationally intensive, we instead focus on the ten best and ten worst runs for each dataset.</p><table><thead><tr><th></th><th>marginal_ll</th><th>n_layers</th><th>n_hidden</th><th>n_latent</th><th>reconstruction_loss</th><th>dropout_rate</th><th>lr</th><th>n_epochs</th><th>n_params</th><th>run index</th></tr></thead><tbody><tr><td>1</td><td>1218.52</td><td>1</td><td>256</td><td>10</td><td>zinb</td><td>0.1</td><td>0.01</td><td>248</td><td>290816</td><td>92</td></tr><tr><td>2</td><td>1218.7</td><td>1</td><td>128</td><td>12</td><td>zinb</td><td>0.1</td><td>0.01</td><td>382</td><td>145920</td><td>80</td></tr><tr><td>3</td><td>1219.7</td><td>1</td><td>256</td><td>10</td><td>zinb</td><td>0.1</td><td>0.01</td><td>365</td><td>290816</td><td>85</td></tr><tr><td>4</td><td>1220.06</td><td>1</td><td>256</td><td>10</td><td>zinb</td><td>0.1</td><td>0.01</td><td>275</td><td>290816</td><td>91</td></tr><tr><td>5</td><td>1223.09</td><td>1</td><td>128</td><td>10</td><td>zinb</td><td>0.1</td><td>0.01</td><td>440</td><td>145408</td><td>83</td></tr><tr><td>6</td><td>1223.2</td><td>1</td><td>128</td><td>12</td><td>zinb</td><td>0.5</td><td>0.005</td><td>703</td><td>145920</td><td>38</td></tr><tr><td>7</td><td>1223.53</td><td>1</td><td>256</td><td>10</td><td>zinb</td><td>0.1</td><td>0.001</td><td>514</td><td>290816</td><td>97</td></tr><tr><td>8</td><td>1223.94</td><td>1</td><td>128</td><td>12</td><td>zinb</td><td>0.5</td><td>0.01</td><td>542</td><td>145920</td><td>74</td></tr><tr><td>9</td><td>1224.37</td><td>1</td><td>128</td><td>12</td><td>zinb</td><td>0.5</td><td>0.01</td><td>524</td><td>145920</td><td>76</td></tr><tr><td>10</td><td>1224.37</td><td>1</td><td>128</td><td>12</td><td>zinb</td><td>0.5</td><td>0.01</td><td>497</td><td>145920</td><td>71</td></tr><tr><td>91</td><td>1554.98</td><td>2</td><td>64</td><td>10</td><td>zinb</td><td>0.7</td><td>0.005</td><td>256</td><td>80896</td><td>45</td></tr><tr><td>92</td><td>1559.01</td><td>5</td><td>64</td><td>7</td><td>nb</td><td>0.5</td><td>0.0001</td><td>457</td><td>105088</td><td>37</td></tr><tr><td>93</td><td>1601.53</td><td>3</td><td>64</td><td>10</td><td>nb</td><td>0.7</td><td>0.001</td><td>88</td><td>89088</td><td>15</td></tr><tr><td>94</td><td>1612.9</td><td>4</td><td>64</td><td>14</td><td>zinb</td><td>0.7</td><td>0.005</td><td>71</td><td>97792</td><td>49</td></tr><tr><td>95</td><td>1615.22</td><td>2</td><td>256</td><td>9</td><td>nb</td><td>0.9</td><td>0.0001</td><td>197</td><td>421376</td><td>20</td></tr><tr><td>96</td><td>1746.25</td><td>3</td><td>128</td><td>12</td><td>zinb</td><td>0.9</td><td>0.001</td><td>134</td><td>211456</td><td>52</td></tr><tr><td>97</td><td>1818.82</td><td>1</td><td>64</td><td>12</td><td>zinb</td><td>0.9</td><td>0.0005</td><td>54</td><td>72960</td><td>60</td></tr><tr><td>98</td><td>6574.57</td><td>1</td><td>128</td><td>8</td><td>zinb</td><td>0.5</td><td>0.0001</td><td>4</td><td>144896</td><td>61</td></tr><tr><td>99</td><td>10680.4</td><td>5</td><td>64</td><td>12</td><td>zinb</td><td>0.3</td><td>0.0001</td><td>2</td><td>105728</td><td>1</td></tr><tr><td>100</td><td>NaN</td><td>2</td><td>64</td><td>6</td><td>zinb</td><td>0.9</td><td>0.0001</td><td>31</td><td>80384</td><td>13</td></tr></tbody></table><p>Table 2: Hyperoptimization results for the Cortex dataset.</p><table><thead><tr><th></th><th>marginal_ll</th><th>n_layers</th><th>n_hidden</th><th>n_latent</th><th>reconstruction_loss</th><th>dropout_rate</th><th>lr</th><th>n_epochs</th><th>n_params</th><th>run index</th></tr></thead><tbody><tr><td>1</td><td>1323.44</td><td>1</td><td>256</td><td>14</td><td>zinb</td><td>0.5</td><td>0.01</td><td>170</td><td>1720320</td><td>70</td></tr><tr><td>2</td><td>1323.88</td><td>1</td><td>256</td><td>8</td><td>zinb</td><td>0.5</td><td>0.01</td><td>130</td><td>1717248</td><td>54</td></tr><tr><td>3</td><td>1324.12</td><td>1</td><td>256</td><td>14</td><td>zinb</td><td>0.5</td><td>0.01</td><td>178</td><td>1720320</td><td>85</td></tr><tr><td>4</td><td>1324.18</td><td>1</td><td>256</td><td>8</td><td>zinb</td><td>0.5</td><td>0.01</td><td>155</td><td>1717248</td><td>65</td></tr><tr><td>5</td><td>1324.2</td><td>1</td><td>256</td><td>15</td><td>zinb</td><td>0.5</td><td>0.01</td><td>140</td><td>1720832</td><td>84</td></tr><tr><td>6</td><td>1324.23</td><td>1</td><td>256</td><td>8</td><td>zinb</td><td>0.5</td><td>0.005</td><td>170</td><td>1717248</td><td>42</td></tr><tr><td>7</td><td>1324.24</td><td>1</td><td>256</td><td>14</td><td>zinb</td><td>0.5</td><td>0.01</td><td>227</td><td>1720320</td><td>87</td></tr><tr><td>8</td><td>1324.25</td><td>1</td><td>256</td><td>8</td><td>zinb</td><td>0.5</td><td>0.01</td><td>176</td><td>1717248</td><td>67</td></tr><tr><td>9</td><td>1324.29</td><td>1</td><td>128</td><td>15</td><td>zinb</td><td>0.3</td><td>0.01</td><td>176</td><td>860416</td><td>73</td></tr><tr><td>10</td><td>1324.32</td><td>1</td><td>256</td><td>8</td><td>zinb</td><td>0.5</td><td>0.01</td><td>133</td><td>1717248</td><td>69</td></tr><tr><td>91</td><td>1347.3</td><td>3</td><td>64</td><td>6</td><td>nb</td><td>0.7</td><td>0.01</td><td>285</td><td>445440</td><td>37</td></tr><tr><td>92</td><td>1350.02</td><td>2</td><td>128</td><td>13</td><td>zinb</td><td>0.9</td><td>0.001</td><td>350</td><td>892672</td><td>8</td></tr><tr><td>93</td><td>1350.54</td><td>1</td><td>64</td><td>5</td><td>zinb</td><td>0.9</td><td>0.0005</td><td>492</td><td>428928</td><td>0</td></tr><tr><td>94</td><td>1350.59</td><td>1</td><td>128</td><td>13</td><td>nb</td><td>0.9</td><td>0.01</td><td>55</td><td>859904</td><td>5</td></tr><tr><td>95</td><td>1350.9</td><td>2</td><td>128</td><td>13</td><td>zinb</td><td>0.9</td><td>0.0005</td><td>710</td><td>892672</td><td>15</td></tr><tr><td>96</td><td>1352.32</td><td>3</td><td>256</td><td>14</td><td>zinb</td><td>0.9</td><td>0.01</td><td>201</td><td>1982464</td><td>88</td></tr><tr><td>97</td><td>1355.68</td><td>3</td><td>256</td><td>5</td><td>zinb</td><td>0.9</td><td>0.001</td><td>668</td><td>1977856</td><td>49</td></tr><tr><td>98</td><td>1365.46</td><td>3</td><td>256</td><td>10</td><td>zinb</td><td>0.9</td><td>0.01</td><td>42</td><td>1980416</td><td>75</td></tr><tr><td>99</td><td>1367.7</td><td>2</td><td>128</td><td>15</td><td>zinb</td><td>0.9</td><td>0.0001</td><td>999</td><td>893184</td><td>13</td></tr><tr><td>100</td><td>1370.3</td><td>3</td><td>256</td><td>8</td><td>nb</td><td>0.9</td><td>0.01</td><td>55</td><td>1979392</td><td>81</td></tr></tbody></table><p>Table 3: Hyperoptimization results for the Pbmc dataset.</p><table><thead><tr><th></th><th>marginal_ll</th><th>n_layers</th><th>n_hidden</th><th>n_latent</th><th>reconstruction_loss</th><th>dropout_rate</th><th>lr</th><th>n_epochs</th><th>n_params</th><th>run index</th></tr></thead><tbody><tr><td>1</td><td>1143.79</td><td>1</td><td>256</td><td>15</td><td>zinb</td><td>0.1</td><td>0.0005</td><td>50</td><td>376320</td><td>92</td></tr><tr><td>2</td><td>1144.11</td><td>1</td><td>256</td><td>15</td><td>nb</td><td>0.1</td><td>0.0005</td><td>42</td><td>376320</td><td>72</td></tr><tr><td>3</td><td>1144.22</td><td>1</td><td>256</td><td>14</td><td>nb</td><td>0.1</td><td>0.005</td><td>48</td><td>375808</td><td>70</td></tr><tr><td>4</td><td>1144.84</td><td>1</td><td>256</td><td>14</td><td>nb</td><td>0.1</td><td>0.005</td><td>43</td><td>375808</td><td>86</td></tr><tr><td>5</td><td>1144.97</td><td>1</td><td>256</td><td>14</td><td>nb</td><td>0.1</td><td>0.005</td><td>48</td><td>375808</td><td>68</td></tr><tr><td>6</td><td>1145.08</td><td>1</td><td>256</td><td>14</td><td>nb</td><td>0.1</td><td>0.005</td><td>43</td><td>375808</td><td>51</td></tr><tr><td>7</td><td>1145.2</td><td>1</td><td>256</td><td>14</td><td>nb</td><td>0.1</td><td>0.005</td><td>47</td><td>375808</td><td>85</td></tr><tr><td>8</td><td>1145.86</td><td>2</td><td>256</td><td>13</td><td>nb</td><td>0.1</td><td>0.0001</td><td>49</td><td>506368</td><td>55</td></tr><tr><td>9</td><td>1146.05</td><td>2</td><td>256</td><td>13</td><td>nb</td><td>0.1</td><td>0.0001</td><td>49</td><td>506368</td><td>42</td></tr><tr><td>10</td><td>1146.11</td><td>2</td><td>256</td><td>13</td><td>nb</td><td>0.1</td><td>0.0001</td><td>49</td><td>506368</td><td>58</td></tr><tr><td>91</td><td>1185.27</td><td>5</td><td>128</td><td>13</td><td>zinb</td><td>0.5</td><td>0.0005</td><td>49</td><td>318720</td><td>40</td></tr><tr><td>92</td><td>1188.06</td><td>5</td><td>256</td><td>14</td><td>nb</td><td>0.7</td><td>0.0001</td><td>44</td><td>900096</td><td>24</td></tr><tr><td>93</td><td>1188.98</td><td>4</td><td>128</td><td>15</td><td>nb</td><td>0.5</td><td>0.005</td><td>49</td><td>286464</td><td>4</td></tr><tr><td>94</td><td>1193.21</td><td>5</td><td>256</td><td>8</td><td>zinb</td><td>0.7</td><td>0.005</td><td>47</td><td>897024</td><td>30</td></tr><tr><td>95</td><td>1202.7</td><td>5</td><td>128</td><td>11</td><td>nb</td><td>0.5</td><td>0.001</td><td>46</td><td>318208</td><td>11</td></tr><tr><td>96</td><td>1203.35</td><td>2</td><td>64</td><td>13</td><td>nb</td><td>0.7</td><td>0.0001</td><td>49</td><td>102016</td><td>56</td></tr><tr><td>97</td><td>1205.6</td><td>2</td><td>64</td><td>9</td><td>nb</td><td>0.7</td><td>0.0001</td><td>48</td><td>101504</td><td>82</td></tr><tr><td>98</td><td>1206.69</td><td>4</td><td>128</td><td>12</td><td>nb</td><td>0.7</td><td>0.0001</td><td>47</td><td>285696</td><td>61</td></tr><tr><td>99</td><td>1211.31</td><td>5</td><td>256</td><td>7</td><td>nb</td><td>0.7</td><td>0.005</td><td>38</td><td>896512</td><td>74</td></tr><tr><td>100</td><td>1232.12</td><td>5</td><td>64</td><td>15</td><td>nb</td><td>0.7</td><td>0.0001</td><td>26</td><td>126848</td><td>52</td></tr></tbody></table><p>Table 4: Hyperoptimization results for the Brain Large dataset.</p><p>From Table 2, 3 and 4 we note that the obtained parameters agree on certain aspects such as the number of layers or the learning rate. Additionally, it is worthwhile noting that the best results often come from configurations with the maximal number of hidden neurons (256), in particular for the PBMC and Brain Large datasets. Perhaps, better performance could be obtained by increasing the search space on this component.</p><p>We purposedly do not use these results to conclude on which type of conditional distribution for the count distribution (NB, ZINB or other; parameterized by <code>reconstruction_loss</code>). Surprisingly, another tuning process run on the PBMC dataset consistently selected the negative binomial loss in the ten best runs. This likely is a bias introduced by the exploration process during the optimization procedure. See our group&#x27;s <a href="https://yoseflab.github.io/2019/06/25/ZeroInflation/" target="_blank" rel="noopener noreferrer">blog post</a> by O. Clivio and P. Boyeau for a complete analysis of the conditional distribution choice.</p><p>Overall, the best performances are stable even though we notice that small changes can have large impacts when we look at the full results (e.g, the number of layers can lead to a sensible difference in held-out log-likelihood). Such stability is in accordance with our extended use of scVI and scANVI in our harmonization preprint <a href="https://www.biorxiv.org/content/10.1101/532895v1" target="_blank" rel="noopener noreferrer">Xu et al. (2019)</a>, where we keep the hyperparameters fixed and obtain competitive performance with state-of-the-art methods. However, we recognize that hyperparameter tuning is an important practice in machine learning method developements and benchmarking and advocate the use of our autotune module for model selection and comparaisons.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="the-case-of-the-brain-large-dataset"></a>The case of the Brain large dataset<a class="hash-link" href="#the-case-of-the-brain-large-dataset" title="Direct link to heading">#</a></h4><p>Given the amount of data available for this dataset, we were expecting to reach optimality with more hidden layers in our neural networks (as shallow networks with bounded width represent a potentially small function class). Suprisingly enough, single-hidden-layer models came out as the best contenders. In Figure 1, we investigate  in further details the relationship between our <code>n_layers</code> parameter and the observed performance in terms of marginal negative log-likelihood.</p><img alt="Synthetic" width="80%" src="/img/blog-post-hyperoptimization/n_layers_brain_large.png"><p>Figure 1: Marginal negative log-likelihood as a function of the number of hidden layers in scVI&#x27;s encoder/decoder, colored by the number of epochs. The number of epochs was limited to 50 or less if the early stopping criterion was triggered.</p><p>It is clear from Figure 1 that the shallow versions of scVI reach better performances on average. Still, a significant portion of hyperparameter configurations reach the maximum number of epochs. Potentially, allowing for a higher budget in terms of the number of epochs could change the outcome of our experiment. Additionally, the fact that we keep only the 720 most variable genes may be too big of a restriction on the amount of signal we keep, making this filtered version of the dataset simple enough to be better and faster captured by a single-hidden-layer model.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="benchmarking"></a>Benchmarking<a class="hash-link" href="#benchmarking" title="Direct link to heading">#</a></h4><p>Our next concern was to investigate the kind of performance uplift we could yield with auto-tuning. Namely, we investigated how optimizing log likelihood could increase other metrics like imputation for example. Our results are reported in the table below.</p><table><thead><tr><th>Dataset</th><th>Run</th><th>Likelihood</th><th></th><th></th><th>Imputation score</th><th></th></tr></thead><tbody><tr><td></td><td></td><td>Marginal ll</td><td>ELBO train</td><td>ELBO test</td><td>Median</td><td>Mean</td></tr><tr><td>cortex</td><td>tuned</td><td>1218.52</td><td>1178.52</td><td>1231.16</td><td>2.08155</td><td>2.87502</td></tr><tr><td></td><td>default</td><td>1256.03</td><td>1224.18</td><td>1274.02</td><td>2.30317</td><td>3.25738</td></tr><tr><td>pbmc</td><td>tuned</td><td>1323.44</td><td>1314.07</td><td>1328.04</td><td>0.83942</td><td>0.924637</td></tr><tr><td></td><td>default</td><td>1327.61</td><td>1309.9</td><td>1334.01</td><td>0.840617</td><td>0.925628</td></tr><tr><td>brain large</td><td>tuned</td><td>1143.79</td><td>1150.27</td><td>1150.7</td><td>1.03542</td><td>1.48743</td></tr><tr><td></td><td>default</td><td>1160.68</td><td>1164.83</td><td>1165.12</td><td>1.04519</td><td>1.48591</td></tr></tbody></table><p>Table 5: Comparison between the best and default runs of scVI on the Cortex, Pbmc and Brain Large datasets. <strong>Held-out</strong> marginal log-likelihood and imputation benchmarks are reported.</p><p>Note that the median (resp. mean) imputation score is the median (resp. mean) of the median absolute error per cell. Also, the held-out marginal negative log-likelihood is an importance sampling estimate of the negative marginal log-likelihood <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">-\log p(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> computed on a 25% test set and is the metric we optimize for.</p><p>From Table 5, we see that the tuning process improves the held-out marginal negative log-likelihood on all three datasets. This model improvement, in turn, ameliorates the imputation metrics for all three datasets, albeit not significantly for the Pbmc and Brain Large datasets.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="efficiency-of-the-tuning-process"></a>Efficiency of the tuning process<a class="hash-link" href="#efficiency-of-the-tuning-process" title="Direct link to heading">#</a></h4><p>Our last concern was to investigate the efficiency of the tuning process. To do so we wanted to check how the held-out negative Evidence Lower BOund (ELBO) histories of the runs were evolving with the number of runs. Below is a plot of these histories.</p><img alt="Synthetic" width="100%" src="/img/blog-post-hyperoptimization/elbo_histories_all.png"><p>Figure 2: Held-out negative ELBO history for each of the 100 trainings performed for each dataset. The lines are colored from red to green where red represents the first trial and green the last one.</p><p>In Figure 2, we see that most green runs (the last trials) are concentrated in the low negative ELBO regions which indicates that the TPE algorithm provides relevant trial suggestions. This would suggest that allowing a slightly higher budget might yield better results. The user should consider this when setting the number of evaluations to run.</p><header><h1>Discussion</h1></header><p>As we illustrated, hyper-parameter tuning provides a improvement for the data fit and also for any metrics that are correlated with the held-out marginal log-likelihood. It is possible that some other scores such as clustering might not be improved by this process since scVI is not per se separating any form of clusters in its latent space. A case where improvement on log-likelihood should be correlated with better clustering scores is for example when using <a href="https://www.biorxiv.org/content/10.1101/532895v1" target="_blank" rel="noopener noreferrer">scANVI</a>, which has a Gaussian mixture model prior over the latent space.</p><p>In this set of experiments, the hyperparameter selection procedure retained for all datasets an architecture with a unique hidden layer. To make sure that scVI benefits from non-linearity (compared to its deterministic homolog <a href="https://www.nature.com/articles/s41467-017-02554-5" target="_blank" rel="noopener noreferrer">ZINB-WaVE</a>), we investigated whether hyperopt would select a model with one layer compared to a linear model. We report our results in <a href="https://docs.google.com/spreadsheets/d/15VenLjjFKrIq1QfSMieKZGkh95TynkIucvL4JH0xGHc/edit?usp=sharing" target="_blank" rel="noopener noreferrer">this table</a> and conclude that a non-linear model sensibly outperforms the linear alternative on both of the Cortex and Pbmc datasets. Brain Large was not part of this experiment as it would have required a considerable amount of additional computing ressources. Besides, it is worth mentioning that some contemporary results tend to indicate that deep architectures are good approximators for gene expression measurements. For instance, <a href="https://academic.oup.com/bioinformatics/article/32/12/1832/1743989" target="_blank" rel="noopener noreferrer">Chen et al. (2016)</a> compare linear regression to their deep neural network D-GEX on gene expression inference. They show architectures with three fully-connected layers outperform shallower architectures and linear regression by a significant margin. Such evidence could very well motivate a more in-depth study of scVI&#x27;s optimization procedure. Perhaps, techniques to improve and accelerate deep neural-networks training could have a significant impact on the results we are able to reach. For example, between-layer connections such as those proposed in the DenseNet architecture <a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener noreferrer">Huang et al. (2018)</a> might lead a path towards future improvement. In the meantime, we also plan to release a patch to scVI allowing for assymetric encoders and decoders as well as variable number of hidden units for each layer of each of scVI&#x27;s neural networks.</p><p>Finally, we have seen how sensible scVI can be with respect to its set of hyperparameters on certain datasets.  Thus, our new hyperparameter tuning feature should be used whenever possible - especially when trying to compare other methods to scVI. We note Qiwen Hu and Casey Greene&#x27;s remarks on <a href="https://www.worldscientific.com/doi/pdf/10.1142/9789813279827_0033?download=true&amp;" target="_blank" rel="noopener noreferrer">hyperparameter selection</a> as well as their <a href="https://www.nature.com/articles/s41592-018-0230-9" target="_blank" rel="noopener noreferrer">comment on Nature Methods</a> and aknowledge their work being our main motivation for incorporating this new tool into scVI.</p><header><h1>Acknowledgements</h1></header><p>We acknowledge Valentine Svensson for proposing the linear decoder and contributing to the scVI codebase.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/v090"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« scvi-tools 0.9.0 release</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/zero-inflation"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Should we zero-inflate scVI? »</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#datasets" class="table-of-contents__link">Datasets</a></li><li><a href="#runtime-information" class="table-of-contents__link">Runtime information</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Yosef Lab, UC Berkeley. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.9e07ed31.js"></script>
<script src="/assets/js/main.3bbe1f1b.js"></script>
</body>
</html>