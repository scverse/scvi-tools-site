<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.6">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="scvi-tools Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="scvi-tools Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><title data-react-helmet="true">Blog | scvi-tools</title><meta data-react-helmet="true" property="og:title" content="Blog | scvi-tools"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" property="og:url" content="https://scvi-tools.org/blog"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_posts_list"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://scvi-tools.org/blog"><link data-react-helmet="true" rel="alternate" href="https://scvi-tools.org/blog" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://scvi-tools.org/blog" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.34d598f1.css">
<link rel="preload" href="/assets/js/runtime~main.25fa66c4.js" as="script">
<link rel="preload" href="/assets/js/main.5868d4e3.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><img src="/img/logo.svg" alt="scvi-tools Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/img/logo.svg" alt="scvi-tools Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">scvi-tools</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/get_started">Get Started</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link">Docs</a><ul class="dropdown__menu"><li><a href="https://docs.scvi-tools.org" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>Full documentation<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/tutorials/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>Tutorials<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/user_guide/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>User guide<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li><a href="https://docs.scvi-tools.org/en/stable/api/index.html" target="_self" rel="noopener noreferrer" class="dropdown__link"><span>API reference<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link">About</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/team">Team</a></li><li><a class="dropdown__link" href="/press">Press</a></li><li><a class="dropdown__link" href="/ecosystem">Ecosystem</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a href="https://discourse.scvi-tools.org/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Discussion<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://github.com/YosefLab/scvi-tools" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_2ahu thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_2hhb margin-bottom--md">Recent posts</div><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/v13">scvi-tools 1.4 release</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/destvi-batchsize">Mini-batch size in destVI</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/v090">scvi-tools 0.9.0 release</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/autotune">Hyperparameter search for scVI</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/blog/zero-inflation">Should we zero-inflate scVI?</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/blog/v13">scvi-tools 1.4 release</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2025-07-03T00:00:00.000Z" itemprop="datePublished">July 3, 2025</time> · 9 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Ori Kronfeld</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h2><p>We’re proud to introduce scvi‑tools v1.4, encompassing major advances in modeling, data loading, computational scalability, metric integration, and interpretability in single-cell analytics. </p><p>Featuring nine new or enhanced models, optimized for spatial, cytometry, methylation, perturbation, and multi‑omic data, it also introduces custom data loaders for large-scale datasets, multi‑GPU model training, on-the-fly metric tuning, and integrated model interpretability. </p><p>This article delves into each enhancement with depth, including detailed insights, illustrative figures, and manuscript references.</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="1--new-models"></a>1. 🔬 New Models<a class="hash-link" href="#1--new-models" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="resolvi"></a>ResolVI<a class="hash-link" href="#resolvi" title="Direct link to heading">#</a></h3><p>ResolVI<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> is a spatial transcriptomics denoising model that reallocates mis-assigned gene counts among true cells, neighborhood leakage, and background. It employs a Gaussian-mixture latent prior to learn corrected counts and interpretable embeddings. This approach is highly scalable (handling &gt;1 million spots) and offers downstream capabilities like differential expression and transfer learning on corrected data</p><p>In <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/spatial/resolVI_tutorial.html" target="_blank" rel="noopener noreferrer">tutorial</a>, it has been shown to markedly enhance spatial expression accuracy in noisy segmentation settings, enabling reliable differential expression especially in high-throughput ST datasets.  </p><img alt="ResolVI" width="100%" src="/img/blog-post-scvi-tools-1p3/resolvi.png"><p>Figure 1: ResolVI cell type annotations based on noisy cellular segmentation Xenium data of a mouse brain. The left hemisphere for model training and the right hemisphere for transfer mapping. </p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="scviva"></a>scVIVA<a class="hash-link" href="#scviva" title="Direct link to heading">#</a></h3><p>scVIVA<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> augments spatial transcriptomics analysis by jointly modeling each cell’s own expression and its micro-environmental context (neighborhood composition and gene counts). This niche-aware VAE embeds both cellular identity and environmental features, revealing tissue-specific patterns and environment-driven variation. Its latent embeddings delineate tissue-specific structures - ideal for spatial differential abundance or niche-focused clustering studies.</p><p>Dedicated <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/spatial/scVIVA_tutorial.html" target="_blank" rel="noopener noreferrer">tutorial</a> showcase how scVIVA enables niche-focused clustering and differential abundance analyses</p><img alt="scVIVA" width="100%" src="/img/blog-post-scvi-tools-1p3/scviva.png"><p>Figure 2: scVIVA results: median Log-Fold Change (LFC) of upregulated genes in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">G1</mtext></mrow><annotation encoding="application/x-tex">\textit{G1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">G1</span></span></span></span></span></span> vs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">G2</mtext></mrow><annotation encoding="application/x-tex">\textit{G2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">G2</span></span></span></span></span></span> displayed on the x-axis, while we compare differential expression computed between <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">N1</mtext></mrow><annotation encoding="application/x-tex">\textit{N1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">N1</span></span></span></span></span></span>  and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">G2</mtext></mrow><annotation encoding="application/x-tex">\textit{G2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">G2</span></span></span></span></span></span> on the y-axis.
Genes are colored by their marker label (yellow=significantly upregulated in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">G1</mtext></mrow><annotation encoding="application/x-tex">\textit{G1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">G1</span></span></span></span></span></span> vs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">N1</mtext></mrow><annotation encoding="application/x-tex">\textit{N1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord text"><span class="mord textit">N1</span></span></span></span></span></span>, green otherwise).
We also display the classifier decision boundary (the predicted probability of being in the yellow class). </p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="cytovi"></a>CytoVI<a class="hash-link" href="#cytovi" title="Direct link to heading">#</a></h3><p>CytoVI<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup> brings totalVI-inspired modeling to cytometry and mass cytometry data. It models protein-marker distributions, corrects for dropouts and technical batch variation, and generates embeddings for downstream clustering and abundance inference. </p><p>Early <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/cytometry/CytoVI_batch_correction_tutorial.html" target="_blank" rel="noopener noreferrer">tutorial</a> already demonstrate clear delineation of immune subpopulations across batch-affected datasets.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="vivs"></a>VIVS<a class="hash-link" href="#vivs" title="Direct link to heading">#</a></h3><p>Variational Inference for Variable Selection (VIVS<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>) identifies associations across modalities such as gene–protein couplings—while rigorously controlling false discovery rates using conditional randomization. VIVS achieves interpretable and scalable feature selection, enabling discovery of biologically meaningful links in paired datasets</p><p>Tutorial in scvi-tools soon to be updated.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="sysvi"></a>SysVI<a class="hash-link" href="#sysvi" title="Direct link to heading">#</a></h3><p>SysVI<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup> tackles major batch effects, such as those arising from cross-species or organoid-versus-tissue studies—using latent cycle-consistency and VampPrior regularization. Compared to Harmony or regular scVI models, SysVI excels at aligning technical systems while preserving true biological variance, producing embedding spaces where analogous cell types across batches cluster coherently.</p><p>in the <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/sysVI.html" target="_blank" rel="noopener noreferrer">tutorial</a>, we show the power of sysVI with data integration between human and mouse immune cells.</p><img alt="sysvi" width="100%" src="/img/blog-post-scvi-tools-1p3/sysvi.png"><p>Figure 3: Example results of integration between human and mouse immune cells </p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="decipher"></a>Decipher<a class="hash-link" href="#decipher" title="Direct link to heading">#</a></h3><p>Decipher<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup> Designed to dissect perturbation effects (e.g., disease versus control), Decipher disentangles shared and condition-specific variation within a VAE framework. <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/decipher_tutorial.html" target="_blank" rel="noopener noreferrer">Demonstrated</a> on AML (acute myeloid leukemia) data, it uncovers latent axes aligning with known disease signatures and identifies corresponding differentially expressed markers - bridging latent space analysis and functional biology.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="methylvi"></a>MethylVI<a class="hash-link" href="#methylvi" title="Direct link to heading">#</a></h3><p>MethylVI<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup> is a VAE tailored for single-cell bisulfite sequencing (scBS‑seq). By modeling methylation probabilities at genomic regions, it captures epigenetic heterogeneity and learns latent spaces that integrate multiple batches. <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scbs/MethylVI_batch.html" target="_blank" rel="noopener noreferrer">Tutorial</a> show it outperforms linear methods like PCA in retaining biologically meaningful methylation structures.</p><img alt="methylvi" width="100%" src="/img/blog-post-scvi-tools-1p3/methylvi.png"><p>Figure 4: MethylVI integration of cell types from different single-cell bi-sulfite sequencing platforms</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="methylanvi"></a>MethylANVI<a class="hash-link" href="#methylanvi" title="Direct link to heading">#</a></h3><p>MethylANVI extends the MethylVI framework with annotation-aware modeling: it jointly integrates methylation profiles with metadata-driven cell-type labels. This supervised model supports both clustering and label transfer, all while capturing latent biological variation across methylome profiles, ideal for epigenetic atlas-building.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="totalanvi"></a>totalANVI<a class="hash-link" href="#totalanvi" title="Direct link to heading">#</a></h3><p>totalANVI brings supervised annotation to CITE‑seq-style multi-omic integration. Leveraging a VAE for RNA + protein, it jointly learns latent embeddings and cell-type classifiers. The model simultaneously performs dimensionality reduction, denoising, differential expression, and accurate cell-type annotation in one cohesive model.</p><p>Tutorial in scvi-tools soon to be updated.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="mrvi-in-pytorch"></a>mrVI in pyTorch<a class="hash-link" href="#mrvi-in-pytorch" title="Direct link to heading">#</a></h3><p>MrVI<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup> (Multi-resolution Variational Inference) is a model written in <a href="https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html" target="_blank" rel="noopener noreferrer">Jax</a> for analyzing multi-sample, multi-batch single-cell RNA-seq data. MrVI is particularly suited for single-cell RNA sequencing datasets with comparable observations across many samples. It conducts both exploratory analyses (locally dividing samples into groups based on molecular properties) and comparative analyses (comparing pre-defined groups of samples in terms of differential expression and differential abundance) at single-cell resolution. </p><p>In recent scvi-tools releases we added a Pytorch implementation of mrVI, to be along with the Jax one, to support broader options on how to use this popular model. </p><p>Tutorial of mrVI in torch running on a subset of <a href="https://doi.org/10.1101/2025.02.20.639398" target="_blank" rel="noopener noreferrer">Tahoe100M</a> cells dataset can be found <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/Tahoe100_mrVI.html" target="_blank" rel="noopener noreferrer">here</a></p><img alt="lamin" width="100%" src="/img/blog-post-scvi-tools-1p3/mrvi.png"><p>Figure 5: mrVI Integration on Covid dataset</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="2-🧩-custom-dataloaders"></a>2. 🧩 Custom Dataloaders<a class="hash-link" href="#2-🧩-custom-dataloaders" title="Direct link to heading">#</a></h2><p>scvi‑tools v1.4 introduces three scalable custom dataloaders: LaminDB, Census, and AnnCollection, enabling out-of-core and federated training without memory overload.
Custom Dataloaders are only supported in SCVI &amp; SCANVI models, but it should be easy to expand them to other models.
These backends support full compatibility with scvi‑tools data registration and training workflows, offering both scale and convenience to large projects.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="lamindb"></a>LaminDB<a class="hash-link" href="#lamindb" title="Direct link to heading">#</a></h3><p>Integrates with <a href="https://lamin.ai/" target="_blank" rel="noopener noreferrer">Lamindb</a>, enabling out-of-core training from disk-backed collections. Users can register collections and seamlessly train models like SCVI using lamin&#x27;s MappedCollection, benefiting from disk efficiency while maintaining full API compatibility with in-memory datasets. For more information see this <a href="https://docs.scvi-tools.org/en/stable/user_guide/use_case/custom_dataloaders.html" target="_blank" rel="noopener noreferrer">link</a>.</p><p>The next <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/custom_dl/lamin.html" target="_blank" rel="noopener noreferrer">tutorial</a> shows demonstration of a scalable approach to training an scVI model on PBMC data using Lamin dataloader.</p><img alt="lamin" width="100%" src="/img/blog-post-scvi-tools-1p3/lamin.png"><p>Figure 6: SCVI Integration achieved using LaminDB dataloader, on 2 distinct PBMC data.</p><p>This <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/custom_dl/Tahoe100_mrVI_lamin.html" target="_blank" rel="noopener noreferrer">tutorial</a> shows the analysis of mrVI in its PyTorch version together with Lamin Custom dataloader over a subset of Tahoe100M cells dataset.</p><img alt="lamin" width="100%" src="/img/blog-post-scvi-tools-1p3/lamin_tahoe100.png"><p>Figure 7: SCVI (bottom) &amp; MRVI (top) Integration achieved using LaminDB dataloader, on a subset of Tahoe100M cells data.</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="census"></a>Census<a class="hash-link" href="#census" title="Direct link to heading">#</a></h3><p>employs <a href="https://www.tiledb.com/" target="_blank" rel="noopener noreferrer">TileDB-SOMA</a> for atlas-scale tensor-backed data, offering similar streaming capabilities but enhanced support for multidimensional genomic inputs and federated study designs.
This custom dataloader directly read cellXgene dataset from S3 and train the SCVI model without the need to first download it, thus very suitable for few shots learning.</p><p>The next <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/custom_dl/tiledb.html" target="_blank" rel="noopener noreferrer">tutorial</a> shows demonstration of a scalable approach to training an scVI model on mus_musculus data using the Census dataloader</p><img alt="census" width="100%" src="/img/blog-post-scvi-tools-1p3/census.png"><p>Figure 8: SCVI Cell Integration achieved using Census dataloader, based on 4 type of batches: dataset_id, donor_id, assay and tissue_general</p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="anncollection"></a>AnnCollection<a class="hash-link" href="#anncollection" title="Direct link to heading">#</a></h3><p>This dataloader allows training on multiple AnnData objects simultaneously, without merging them into one dataset. AnnCollection handles disparities in features or layers internally and aligns them during training, empowering federated or multi-study analyses</p><p>The next <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/custom_dl/ann_collection.html" target="_blank" rel="noopener noreferrer">tutorial</a> shows how to apply the annCollection wrapper in scvi-tools to load and train SCANVI model on several adata&#x27;s that are stored on disk.
Another <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/hub/Tahoe100.html" target="_blank" rel="noopener noreferrer">link</a> shows how the <a href="https://doi.org/10.1101/2025.02.20.639398" target="_blank" rel="noopener noreferrer">Tahoe100M</a> cells dataset was trained in SCVI using the annCollection wrapper and its minified version was stored on scvi-hub for further analysis.</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="3-️-core-enhancements"></a>3. ⚙️ Core Enhancements<a class="hash-link" href="#3-️-core-enhancements" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="multigpu-training"></a>Multi‑GPU Training<a class="hash-link" href="#multigpu-training" title="Direct link to heading">#</a></h3><p>Built on PyTorch Lightning, v1.4 empowers all major models to run across multiple GPUs with a single API flag. Training benchmarks times reduced by number of GPU exists, with full gradient synchronization and no code modifications needed.
See the following <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/use_cases/multiGPU.html" target="_blank" rel="noopener noreferrer">tutorial</a> and info <a href="https://docs.scvi-tools.org/en/stable/user_guide/use_case/multi_gpu_training.html" target="_blank" rel="noopener noreferrer">page</a></p><img alt="multi-gpu" width="100%" src="/img/blog-post-scvi-tools-1p3/multigpu.png"><p>Figure 9: comparison of SCVI training time between single and X2 multi-GPU machines as data increase. </p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="scibmetrics-optimization"></a>scIB‑Metrics Optimization<a class="hash-link" href="#scibmetrics-optimization" title="Direct link to heading">#</a></h3><p>With the integration of <code>ScibCallback</code> and the <code>AutotuneExperiment</code> class, users can now monitor <a href="https://github.com/YosefLab/scib-metrics" target="_blank" rel="noopener noreferrer">scIB metrics</a> on the validation set during training and automatically tune hyperparameters (model, training and architecture parameters) based on these metrics—directly optimizing for clustering and batch mixing performance, making the model training process more principled and outcome-driven.
See the following <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/use_cases/autotune_scvi.html#tuning-over-integration-metrics-with-scib-metrics" target="_blank" rel="noopener noreferrer">tutorial</a></p><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="explainability--interpretability"></a>Explainability &amp; Interpretability<a class="hash-link" href="#explainability--interpretability" title="Direct link to heading">#</a></h3><p>v1.4 brings native support for <a href="https://captum.ai/api/integrated_gradients.html" target="_blank" rel="noopener noreferrer">Captum&#x27;s</a> Integrated Gradients (IG) across semi-supervised generative models like Scanvi and totalANVI. Users can compute marker-level attribution scores tied to latent dimensions or differential axes. This complements <code>get_normalized_expression</code>, delivering a full pipeline that links model representations back to biologically interpretable molecular mechanisms, offering transparency and interpretability in deep generative modeling</p><p>See the following <a href="https://docs.scvi-tools.org/en/stable/tutorials/notebooks/use_cases/interpretability.html" target="_blank" rel="noopener noreferrer">tutorial</a> as an example of scanvi model ran on a PBMC dataset from 10X.</p><img alt="ig" width="100%" src="/img/blog-post-scvi-tools-1p3/ig.png"><p>Figure 10: Integrated gradients total contribution per gene per cell type, over data of PBMC.</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><p>scvi‑tools v1.4 is a landmark release that advances the field across three foundational pillars:</p><ol><li><p><strong>Innovative modeling</strong> across ten tailored VAEs for spatial, protein, methylation, perturbation, and multi‑omic data.</p></li><li><p><strong>Scalable data processing</strong> with three new custom dataloaders in the backend, enabling efficient handling of federated, out-of-core, atlas-scale datasets, such as the Tahoe100M cells.</p></li><li><p><strong>Infrastructure and transparency</strong> with multi-GPU training, metric-aware tuning, and demonstrable model interpretability.</p></li></ol><p>Together, these developments empower researchers to build, train, and interpret probabilistic models at scale-in a reproducible, transparent, and biologically meaningful way.</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="references"></a>References<a class="hash-link" href="#references" title="Direct link to heading">#</a></h2><div class="footnotes"><hr><ol><li id="fn-1">ResolVI: addressing noise and bias in spatial transcriptomics / Ergen et al.<a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2">scVIVA: a probabilistic framework for representation of cells and their environments in spatial transcriptomics / Levy et al.<a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3">CytoVI: Deep generative modeling of antibody-based single cell technologies / Ingelfinger et al.<a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4">VI-VS: calibrated identification of feature dependencies in single-cell multiomics / Boyeau et al.<a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5">sysVI: Integrating single-cell RNA-seq datasets with substantial batch effects / Hrovatin et al.<a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6">Decipher: Joint representation and visualization of derailed cell states with Decipher / Nazaret et al.<a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7">MethylVI: A deep generative model of single-cell methylomic data / Weinberger et al.<a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8">MrVI: Deep generative modeling of sample-level heterogeneity in single-cell genomics / Boyeau et al.<a href="#fnref-8" class="footnote-backref">↩</a></li></ol></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/scvi-tools">scvi-tools</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/release">release</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/blog/destvi-batchsize">Mini-batch size in destVI</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2022-05-29T00:00:00.000Z" itemprop="datePublished">May 29, 2022</time> · 15 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Can Ergen, Romain Lopez, Nir Yosef</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The task of deconvolution of spot-based spatial transcriptomics (ST) data consists in finding the cellular composition in spots of ST assays. Indeed, by design each spot consists of several individual cells. Our lab developed destVI as a tool to study cell type composition of spots (1). In the case where one expects within cell type variation (i.e., activation states), DestVI was designed to infer the activation state of individual cell types in each spot and therefore gives additional resolution of cell composition over competing algorithms. In our own benchmarking of destVI, we found comparable performance in prediction of cell type proportions to other state of the art algorithms like Cell2Location (2). A recent benchmarking study compared the performance of several gene imputation as well as deconvolution methods. Given the ever increasing number of deconvolution methods, the effort to benchmark those tools on a variety of simulated data is timely (3). In two of the benchmarking cases, DestVI had the worst performance. Along with the recent acceptance of the manuscript, we published several fixes to the codebase in scvi-tools v0.16.0, repeated those experiments and analyzed the reasons for failure of spot deconvolution in the given experiments. Briefly, we show that the poor performance was due to the mini-batch size used during optimization, and resolving this help recovering competitive accuracy. Additionally, we provide the user with a heuristic for future use.</p><p>More specifically, variational autoencoders are a specific type of neural networks (NN) that, like most NN, are trained by iteratively presenting the network mini-batches of data, that is a small fraction of the whole dataset. The mini-batch size refers to number of distinct samples used for updating the parameters of the neural networks during one training step.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="methodology"></a>Methodology<a class="hash-link" href="#methodology" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="models"></a>Models<a class="hash-link" href="#models" title="Direct link to heading">#</a></h3><p>The DestVI model (1) uses a first training phase on a reference single cell dataset to learn a cell-type specific latent space. Then, it learns a second model using as input the spot data, and encoding them in the reference latent space. Finally, the model learns cell-type proportions for each spot. In this blog post, we only address the cell-type proportion estimate as was done in the original benchmarking study (3). We haven&#x27;t checked the activation state of those cells as the simulation was designed in a way where the mean activation state of each cell-type in each spot is not known and ground-truth therefore is missing. For all experiments here, we kept the parameters for the CondSCVI model on which the single-cell data is learned constant and highlight every parameter we changed in the respective run. The only two parameters that we highlight here are the amortization scheme, where destVI allows a neural network for amortization of cell-type proportions and activation state or models them as free parameters, and the mini-batch size used for training.</p><p>The selection of genes for destVI used in the original benchmarking study is a point we wanted to mention here. The authors first took the intersection of the genes in spatial assay and single cell assay (882 genes overlapping for STARmap dataset) and then took the 2000 highly variable genes. This second step was without effect (2000 is higher than 882). Nevertheless, in case a FISH based experiments was designed specifically for an organ and all genes were carefully selected, we generally would recommend against additional filtering for over-dispersed genes in a single cell reference but train the model directly on the overlap of spatial and single cell genes skipping the step of highly variable genes.</p><p>We compare destVI throughout this blog post with the benchmark version of Cell2Location. Our purpose is not to prove that we outperform other existing methods but to analyze why destVI showed poor performance in the benchmarking study. Cell2Location showed overall good performance in the benchmarking study and is implemented using the scvi-tools framework but relies on a marker gene related approach instead of an unbiased deconvolution approach.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="hyperparameter-selection"></a>Hyperparameter selection<a class="hash-link" href="#hyperparameter-selection" title="Direct link to heading">#</a></h3><p>Most experiments before the destVI publication, as well as the benchmarking study were performed using cell-type activation state amortization only (called latent amortization hereafter) and treat the cell-type proportions as a free parameter. However, in the recent version of the code we generally recommend to use the amortized version of both (called both amortization hereafter). In this blog post, we checked performance in all experiments here for both amortization schemes. The size of the training mini-batch size varied as {4, 8, 16, 32, 64, 128, 256, 512, 1024}. For comparison with the benchmarking study we left out batch sizes 256 or higher because the number of spots was 189. For our comparison, in a large scale dataset we left out batch_sizes 16 and lower because the training time drastically increases when using small batch sizes on large datasets.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="results"></a>Results<a class="hash-link" href="#results" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="datasets"></a>Datasets<a class="hash-link" href="#datasets" title="Direct link to heading">#</a></h3><p>We reused the STARmap dataset from the original publication (mouse brain cortex). The single-cell reference contains 14,249 cells by 34,041 genes. It contains data from several mouse lines, both sexes and from various time points that were sorted by flow cytometry and sequentially sequenced. The STARmap dataset contains 1,523 cells by 981 genes. The simulation is to sum all counts over a window size of 750 pixels. This gives a pseudo-spot count matrix with 189 spots and 1-17 cells per spot. For further reference, we refer to the original publication (3). We subset this dataset to 57 spots to see how destVI performs in the case of even lower number of spots by subsetting this data to the first three column as diversity of cell-types is mainly along the cortical axis and subsetting to columns keeps the complexity of the dataset similar.</p><p>For a regime with more spots, we decided to keep the organ of interest (mouse brain) and use a dataset with a much higher number of cell captured. We reproduced for this matter the analysis provided by Vizgen using MERFISH technology, which provides a walk-through tutorial on Google Colab.</p><img alt="MERFISH_data" width="100%" src="/img/blog-post-destvi-batchsize/vizgen_colab.png"><p>Figure 1: Overview of MERFISH brain dataset from (<a href="https://colab.research.google.com/drive/1OxJRO19cPsDW0JGAh4tLJjgOl7EMxQbP?usp=sharing&amp;__hstc=30510752.37206d737856c71bb0a5d1c8f6764b63.1652985789816.1653807477271.1653882474080.8&amp;__hssc=30510752.1.1653882474080&amp;__hsfp=455698764&amp;hsCtaTracking=070f4af1-2595-44c8-9779-4da89d538482%7Cf4313de5-25c4-4677-9fd6-82cf71d4fdc4" target="_blank" rel="noopener noreferrer">https://colab.research.google.com/drive/1OxJRO19cPsDW0JGAh4tLJjgOl7EMxQbP?usp=sharing&amp;__hstc=30510752.37206d737856c71bb0a5d1c8f6764b63.1652985789816.1653807477271.1653882474080.8&amp;__hssc=30510752.1.1653882474080&amp;__hsfp=455698764&amp;hsCtaTracking=070f4af1-2595-44c8-9779-4da89d538482%7Cf4313de5-25c4-4677-9fd6-82cf71d4fdc4</a>).</p><p>This analysis yields a single-cell reference dataset with 160,796 cells by 27,998 genes and a spatial dataset with 83,546 cells by 483 genes. The simulation here is again to sum all counts of cells with a center over a window of 40 µm. We choose the size of this window to have an equal number of cells per pseudo-spot compared to the STARmap dataset. This gives a pseudo-spot count matrix with 27395 spots and 1-16 cells per spot.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="results-on-starmap-dataset"></a>Results on STARmap dataset<a class="hash-link" href="#results-on-starmap-dataset" title="Direct link to heading">#</a></h3><p>First, we verified that by using the updated version of the code in scvi-tools v0.16.0, we get similar results to the benchmarking study. In agreement with the original benchmarking study, the layer structure of neurons in different cortical layers wasn&#x27;t visible.</p><p>Then, we ran different amortization variants of DestVI. Modeling cell-type proportions as a free parameter leads to no visible structure at all (as was run by the study). When using amortized cell-type proportions, which uses a neural network to estimate cell-type proportions, destVI predicted astrocytes correctly, while it wasn&#x27;t capable of differentiating different excitatory neurons but were classifying all neurons as a single mixture.</p><p>By changing the batch-size, we noticed that decreasing the training mini-batch size drastically improves the performance of both algorithms.  Notably, the model with a mini-batch size of 32 and both parameters amortized performs badly. We see good deconvolution of the different neuronal layers with a batch size of 8, 12 and 16. Additionally, for these batch size there was no qualitative difference between both amortization and latent amortization.</p><p>When comparing the results of destVI with Cell2Location it becomes clear that Cell2Location outperforms destVI for cell-types like Pvalp or Smc cells while both algorithms fail on microglia. The reason for better performance of Cell2Location is most likely low number of those cell-types in the spatial dataset and therefore low percentage in the respective spot. The bad performance for microglia might be based on the selection of FISH probes giving a low coverage of myeloid cell heterogeneity.</p><img alt="STARmap" width="100%" src="/img/blog-post-destvi-batchsize/STARmap.png"><p>Figure 2: Results on benchmarking dataset. Left-to-right ground truth, Cell2Location, DestVI with both amortization and batch-size 8, 16, 48, 128 and DestVI with latent amortization with same batch size. Increase in matching proportions for destVI with decreasing mini-batch size. Cell2Location outperforms destVI for Oligodendrocytes. Quantitative measurues (PCC=Pearson Correlation Coefficient, SSIM=Structural Similarity, RMSE=Root Mean Squared Error, JSD=Jensen-Shannon-Divergence) show on par performance for destVI with a mini-batch size below 16.</p><table><thead><tr><th>cell-type</th><th>freq_sc</th><th>freq_spatial</th></tr></thead><tbody><tr><td>ExcitatoryL6</td><td>3190</td><td>287</td></tr><tr><td>ExcitatoryL5</td><td>1786</td><td>94</td></tr><tr><td>Sst</td><td>1741</td><td>42</td></tr><tr><td>Vip</td><td>1728</td><td>15</td></tr><tr><td>ExcitatoryL4</td><td>1401</td><td>198</td></tr><tr><td>Pvalb</td><td>1337</td><td>42</td></tr><tr><td>ExcitatoryL2and3</td><td>982</td><td>258</td></tr><tr><td>Astro</td><td>368</td><td>141</td></tr><tr><td>Endo</td><td>94</td><td>150</td></tr><tr><td>Olig</td><td>91</td><td>200</td></tr><tr><td>Smc</td><td>55</td><td>13</td></tr><tr><td>Micro</td><td>51</td><td>23</td></tr></tbody></table><p>When checking quantitative results, we find on par performance of destVI and Cell2Location in Pearson Correlation Coefficient based on spots while when checking for correlation across cell-types Cell2Location outperforms destVI. The reason for this improved performance are as described above lowly abundant cell-types.</p><p>As demonstrated here, by reducing the size of the training mini-batch destVI yields overall similar performance to Cell2Location for cell-type deconvolution. We asked next whether this is also the case when even further reducing the number of spots. For this study, as described above we subset the number of pseudo-spots and retrained Cell2Location and destVI. Overall, we find better agreement with both amortization for different sizes of training batch size. Oligodendrocytes and Astrocytes are correctly predicted in all version with both amortization. Only the models with a batch size of 4 differentiate between the different layers of excitatory neurons. Of note, latent amortization outperforms both amortization here for a mini-batch size of 4. It might be an effect of small training size, so that the amortization network can not be trained well. We generally wouldn&#x27;t recommend to use spatial deconvolution techniques for such low number of spots. Given the higher stability over a various number of mini-batch sizes, we prefer to recommend using both amortization scheme. In cases with very few examples and known ground-truth we advise training both models and comparing the results.</p><img alt="STARmap_sub" width="100%" src="/img/blog-post-destvi-batchsize/STARmap_sub.png"><p>Figure 3: Results on subset of benchmarking dataset. Subset on first three columns in original dataset. Displayed are only neuron layers as structure in other celltypes is hardly detected with only three columns. For ground-truth, we display all columns to allow easier comparison. Left-to-right ground truth, Cell2Location, DestVI with both amortization and batch-size 4, 8, 12, 16, 32 and DestVI with latent amortization with same batch size. On par performance with mini-batch size 4 and latent amortization is visible with slightly reduced performance in both amortization.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_31ik" id="results-on-merfish-dataset"></a>Results on MERFISH dataset<a class="hash-link" href="#results-on-merfish-dataset" title="Direct link to heading">#</a></h3><p>To check the effect of training mini-batch size on the performance of large datasets, we simulated a second pseudospot matrix. The training time increases drastically with reducing the mini-batch size as the GPU is used less efficiently. We therefore restricted to models with a mini-batch size above 32 (trained more than 3 hours on a Nvidia RTX 3090). We therefore think that the mini-batch size should be 128, in which case we haven&#x27;t seen major speed improvement (most likely depends on the GPU architecture).</p><table><thead><tr><th>model</th><th>computation time</th></tr></thead><tbody><tr><td>Cell2location</td><td>2h 3min 13s</td></tr><tr><td>batchsize_32 both_amortization</td><td>3h 31min 27s</td></tr><tr><td>batchsize_48 both_amortization</td><td>1h 44min 21s</td></tr><tr><td>batchsize_64 both_amortization</td><td>1h 26min 5s</td></tr><tr><td>batchsize_128 both_amortization</td><td>36min 17s</td></tr><tr><td>batchsize_256 both_amortization</td><td>20min 48s</td></tr><tr><td>batchsize_512 both_amortization</td><td>10min 55s</td></tr><tr><td>batchsize_1024 both_amortization</td><td>10min 03s</td></tr></tbody></table><p>Overall, for all combinations of parameters we see improved performance of destVI over standard Cell2Location. This is especially visible in Di- and mesencephalon excitatory neurons and Telencephalon inhibitory inter-neurons where Cell2Location doesn&#x27;t uncover the tissue distribution of this cell-type. We see here no correlation to small size of those cell-types, and the reason for this reduced performance is not clear. As we haven&#x27;t set out a benchmarking study here, but to study the performance of destVI, we haven&#x27;t changed the hyper-parameters for Cell2Location to increase performance.</p><table><thead><tr><th>cell-type</th><th>freq_sc</th><th>freq_spatial</th></tr></thead><tbody><tr><td>Oligodendrocytes</td><td>30253</td><td>10244</td></tr><tr><td>Astrocytes</td><td>19377</td><td>9476</td></tr><tr><td>Telencephalon projecting exc. neurons</td><td>18799</td><td>22345</td></tr><tr><td>Telencephalon inh. interneurons</td><td>8637</td><td>4451</td></tr><tr><td>Mesencephalon exc. neurons</td><td>6455</td><td>8066</td></tr><tr><td>TE proj. inh. neurons</td><td>5691</td><td>3569</td></tr><tr><td>Microglia</td><td>5425</td><td>477</td></tr><tr><td>Vascular endothelial cells</td><td>3805</td><td>6188</td></tr><tr><td>Vascular smooth muscle cells</td><td>1628</td><td>2018</td></tr><tr><td>Vascular and leptomeningeal cells</td><td>1501</td><td>1905</td></tr><tr><td>Ependymal cells</td><td>1257</td><td>900</td></tr><tr><td>Hindbrain neurons</td><td>1144</td><td>43</td></tr><tr><td>Cholinergic and monoaminergic neurons</td><td>1071</td><td>6163</td></tr><tr><td>Oligodendrocyte precursor cells</td><td>820</td><td>4524</td></tr><tr><td>Choroid epithelial cells</td><td>458</td><td>477</td></tr></tbody></table><p>Overall we see that performance is stable up to a batch size of 256 with decreasing performance for both amortization and batch size 512 and 1,024, while performance of latent amortization is stable with increasing batch size. We postulate that mini-batch training of the cell-type amortization network is essential for performance. As above we have seen speed improvement by using a bigger batch size, we asked whether bigger batch sizes are good in performance, when we train them for more epochs. Indeed increasing the number of epochs for mini-batch size 512 lead to on par performance using 5,000 instead of the default 2,500 training epochs, while performance of mini-batch size 1,024 was still inferior when checking with 10,000 training epochs.</p><img alt="MERFISH" width="100%" src="/img/blog-post-destvi-batchsize/MERFISH2.png"><p>Figure 4: Results on MERFISH brain dataset. Left-to-right ground truth, Cell2Location, DestVI with both amortization and batch-size 32, 128, 256, 1,024 and DestVI with latent amortization with same batch size. Cell type proportion estimates are improved over Cell2Location in all destVI models. There is a decrease in performance for models with batch_size 1,024 for endothelial cells, that are low abundant in every spot.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="conclusion"></a>Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">#</a></h2><p>In the analysis of destVI from the main paper, we had limited our benchmarking to standard spot based assays, in which both spatial and single cell data sets are based on whole transcriptome sequencing and especially contain more than 1,000 spots. We found that DestVI performance from the newly published benchmarking study was mediocre because the number of spots was close to the training mini-batch size and therefore the underlying composition of the spots was not learned adequately. We verified this by proving that by decreasing training mini-batch size, destVI can yield on par performance to other methods for cell type deconvolution.</p><p>DestVI yields not only cell-type proportion estimates but also cell-type activation estimates, the benchmarking study was designed to only study cell-type proportion estimates and we kept the same design here. Generally, we think the additional output of cell-type activation is a major benefit of DestVI over competing algorithms. We nevertheless thank the authors of the original benchmarking study to discover deficiencies of destVI with small number of spots.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor anchor__h4 anchorWithStickyNavbar_31ik" id="practical-recommendations"></a>Practical recommendations<a class="hash-link" href="#practical-recommendations" title="Direct link to heading">#</a></h4><p>We demonstrated that destVI also yields those results with a subset of the original dataset with just 57 spots. We also checked 19 spots here and destVI and Cell2Location weren&#x27;t discovering the different layers of cortical neurons. We therefore recommend users to not run destVI with less than 50 spots.</p><p>Over the course of our experiments, we found a training mini-batch size of max(dataset_size/10, 128) to perform well in deconvolution. We set the maximum batch size to 128 as we saw decreasing performance with a batch size of 512 for the brain dataset. Most likely it is safe to increase the mini-batch size for big datasets and getting runtime benefits. However, we have most experience from experiments with a batch size of 128 and limit the maximum batch size to this value. If runtime is a big concern, manual increase of this parameter is possible. The version with batchsize 128 was already several times faster than Cell2Location.</p><p>DestVI with latent amortization showed superior performance in the setting with optimal mini-batch size in small datasets but performance was inferior for other mini-batch sizes. We continue suggesting both amortization and in the case of the brain dataset both amortization schemes were similar in performance.</p><p>Please share any feedback with us via twitter (@YosefLab), through the comment section below or through scverse discourse webpage (<a href="https://discourse.scverse.org/" target="_blank" rel="noopener noreferrer">https://discourse.scverse.org/</a>).</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="acknowledgements"></a>Acknowledgements<a class="hash-link" href="#acknowledgements" title="Direct link to heading">#</a></h2><p>We acknowledge members of the Yosef Lab. We thank Adam Gayoso for reviewing the changes to destVI and bringing the benchmarking study to our attention.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_31ik" id="bibliography"></a>Bibliography<a class="hash-link" href="#bibliography" title="Direct link to heading">#</a></h2><p>(1) Romain Lopez, Baoguo Li, Hadas Keren-Shaul, Pierre Boyeau, Merav Kedmi, David Pilzer, Adam Jelinski, Ido Yofe, Eyal David, Allon Wagner, Can Ergen, Yoseph Addadi, Ofra Golani, Franca Ronchese, Michael I. Jordan, Ido Amit and Nir Yosef. DestVI identifies continuums of cell types in spatial transcriptomics data. Nature Biotechnology. 2022.</p><p>(2) Vitalii Kleshchevnikov, Artem Shmatko, Emma Dann, Alexander Aivazidis, Hamish W. King, Tong Li, Rasa Elmentaite, Artem Lomakin, Veronika Kedlian, Adam Gayoso, Mika Sarkin Jain, Jun Sung Park, Lauma Ramona, Elizabeth Tuck, Anna Arutyunyan, Roser Vento-Tormo, Moritz Gerstung, Louisa James, Oliver Stegle and Omer Ali Bayraktar.  Cell2location maps fine-grained cell types in spatial transcriptomics. Nature Biotechnology. 2022.</p><p>(3) Bin Li, Wen Zhang, Chuang Guo, Hao Xu, Longfei Li, Minghao Fang, Yinlei Hu, Xinye Zhang, Xinfeng Yao, Meifang Tang, Ke Liu, Xuetong Zhao, Jun Lin, Linzhao Cheng, Falai Chen, Tian Xue and Kun Qu. Benchmarking spatial and single-cell transcriptomics integration methods for transcript distribution prediction and cell type deconvolution. Nature Methods. 2022.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/destvi">destvi</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/batch-size">batch-size</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/blog/v090">scvi-tools 0.9.0 release</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2021-03-03T00:00:00.000Z" itemprop="datePublished">March 3, 2021</time> · 7 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Adam Gayoso, Romain Lopez, Galen Xing, Nir Yosef</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Today we officially released <code>scvi-tools</code> version 0.9.0 (<a href="https://docs.scvi-tools.org/en/stable/release_notes/index.html" target="_blank" rel="noopener noreferrer">changelog</a>). This release marks the culmination of five months of work on the backend of the codebase, which came after three months of work on the frontend.
In this short note, we officially introduce <code>scvi-tools</code> as a readily usable codebase that contains many implementations of probabilistic single-cell omics methods, and also features a high-level interface to accelerate the model development process. We start with some historical notes about our previous codebase, which was mostly used for internal developments in the last three years. We then describe the obstacles we found to its external adoption, and the foundational idea behind the new <code>scvi-tools</code> work: a high-level deep probabilistic programming library specialized for single-cell omics data.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/scvi-tools">scvi-tools</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/release">release</a></li></ul></div><div class="col col--3 text--right"><a aria-label="Read more about scvi-tools 0.9.0 release" href="/blog/v090"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/blog/autotune">Hyperparameter search for scVI</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2019-07-05T00:00:00.000Z" itemprop="datePublished">July 5, 2019</time> · 20 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Gabriel Misrachi, Jeffrey Regier, Romain Lopez, Nir Yosef</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>While stochastic gradient-based optimization is highly successful for setting weights and other differentiable parameters of a neural network, it is in general useless for setting hyperparameters -- non-differentiable parameters that control the structure of the network (e.g. the number of hidden layers, or the dropout rate) or settings of the optimizer itself (e.g., the learning rate schedule). Yet finding good settings for hyperparameters is essential for good performance for deep methods like <a href="https://www.nature.com/articles/s41592-018-0229-2" target="_blank" rel="noopener noreferrer">scVI</a>. Furthermore, as pointed out by <a href="https://www.worldscientific.com/doi/pdf/10.1142/9789813279827_0033?download=true&amp;" target="_blank" rel="noopener noreferrer">Hu and Greene (2019)</a> selecting hyperparameters is nessary in order to compare different machine learning models, especially if those are substantially sensitive to hyperparameter variations.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--3 text--right"><a aria-label="Read more about Hyperparameter search for scVI" href="/blog/autotune"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/blog/zero-inflation">Should we zero-inflate scVI?</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2019-06-25T00:00:00.000Z" itemprop="datePublished">June 25, 2019</time> · 22 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a itemprop="url"><span itemprop="name">Oscar Clivio, Pierre Boyeau, Romain Lopez, Jeffrey Regier, Nir Yosef</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Droplet- based single-cell RNA sequencing (scRNA-seq) datasets typically contain at least 90% zero entries. How can we best model these zeros? Recent work focused on modeling zeros with a mixture of count distributions. The first component is meant to reflect whether such an entry can be explained solely by the limited amount of sampling (on average ~5% or less of the molecules in the cell). The second component is generally used to reflect &quot;surprising&quot; zeros caused by measurement bias, transient transcriptional noise (e.g., &quot;bursty&quot; gene with a short mRNA half life), or true longer-term heterogeneity that can not be captured by a similified (low dimensional) representation of the data. Among others, zero-inflated distributions (i.e., zero-inflated negative binomial) have been widely adopted to model gene expression levels (1, 2).</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/autozi">autozi</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/blog/tags/zero-inflation">zero-inflation</a></li></ul></div><div class="col col--3 text--right"><a aria-label="Read more about Should we zero-inflate scVI?" href="/blog/zero-inflation"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025, Yosef Lab, Weizmann Institute of Science. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.25fa66c4.js"></script>
<script src="/assets/js/main.5868d4e3.js"></script>
</body>
</html>